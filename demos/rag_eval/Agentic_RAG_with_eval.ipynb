{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 4.5: Agentic RAG with reference and LM-Eval Eval\n",
    "\n",
    "This tutorial presents an example of evaluating an agentic RAG in LLama-Stack using the reference implementation and a custom\n",
    "provider using the [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) tool,\n",
    "aka `LM-Eval`. \n",
    "\n",
    "Please refer to `# Level4_agentic_RAG.ipynb` [notebook](../rag_agentic/notebooks/Level4_RAG_agent.ipynb)\n",
    "for details on how to initialize the agent and the knowledge search RAG tool provided by Llama Stack.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Connecting to a llama-stack server.\n",
    "2. Indexing a collection of documents in a vector DB for later retrieval.\n",
    "3. Initializing the agent capable of retrieving content from vector DB via tool use.\n",
    "4. Evaluating the agent responses against a reference set of Q&A.\n",
    "5. Reporting the evaluation results and its statistical relevance.\n",
    "\n",
    "## Case study\n",
    "For the purpose of this training, we are going to use the fictional company \n",
    "[Parasol Financial](https://www.redhat.com/en/blog/ai-insurance-industry-insights-red-hat-summit-2024), and the provided\n",
    "[training documents](https://github.com/jharmison-redhat/parasol-financial-data/).\n",
    "\n",
    "A sample Q&A document is available as a [reference](./data/parasol-financial-data_qac.yaml). \n",
    "This predefined question and answer pairs have beeen generated using [docling-sdg](https://github.com/docling-project/docling-sdg),\n",
    "an IBM set of tools to create artificial data from documents, leveraging generative AI and Docling's parsing capabilities.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have a running instance of the Llama Stack server (local or remote) with at least one preconfigured vector DB. For more information, please refer to the corresponding [Llama Stack tutorials](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).\n",
    "\n",
    "The `openai` inference provider is required if you intend to use an OpenAI model for judging purposes, like `openai/gpt-4o`. In this case, the \n",
    "`OPENAI_API_KEY` env variable must be configured into the Llama Stack server.\n",
    "\n",
    "**Notes**:\n",
    "* In order to run the evaluation steps with the Llama Stack reference implementation, the recommended deployment is the one \n",
    "  available in `kubernetes/kustomize/overlay/eval`.\n",
    "* To run the evaluation steps with the LM-Eval implementation, the recommended deployment is the one \n",
    "  available in `kubernetes/kustomize/overlay/lmeval` (which also includes the above requirements).\n",
    "\n",
    "## Setting the Environment Variables\n",
    "\n",
    "Use the [`.env.example`](../../../.env.example) to create a new file called `.env` and ensure you add all the relevant environment variables below.\n",
    "\n",
    "In addition to the environment variables listed in the [\"Getting Started\" notebook](../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb), the following should be provided for this demo to run:\n",
    " - `LLM_AS_JUDGE_MODEL_ID`: the model to use as the judge to evaluate the agent responses. Must be one of the models defined in Llama Stack.\n",
    " - `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    " - `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    " - `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    " - `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "We will start with a few imports needed for this demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "854e7cb4-aed9-4098-adc1-a66f4c9e6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "from llama_stack_client import Agent, AgentEventLogger, RAGDocument\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab4244-e7af-405b-b0c3-4bf00411f26e",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b87b139-bd18-47b2-889a-1b8ed3018655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n",
      "Inference Parameters:\n",
      "\tModel: granite32-8b\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n",
      "Eval Parameters:\n",
      "\tJudge Model: openai/gpt-4o\n",
      "\tQ&A file: ./data/parasol-financial-data_qac.yaml\n",
      "\tMax rows: 50\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "# to override the judge model\n",
    "from llama_stack.providers.inline.scoring.llm_as_judge.scoring_fn.fn_defs.llm_as_judge_405b_simpleqa import (\n",
    "    llm_as_judge_405b_simpleqa,\n",
    ")\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from rag_agentic.src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "remote = os.getenv(\"REMOTE\", \"True\")\n",
    "\n",
    "if remote == \"False\":\n",
    "    local_port = os.getenv(\"LOCAL_SERVER_PORT\", 8321)\n",
    "    base_url = f\"http://localhost:{local_port}\"\n",
    "else: # any value non equal to 'False' will be considered as 'True'\n",
    "    base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=None\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server @ {base_url}\")\n",
    "\n",
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "# The Q&A file\n",
    "QNA_FILE = './data/parasol-financial-data_qac.yaml'\n",
    "# The number of rows to consider\n",
    "MAX_QNA_ROWS = 50\n",
    "# Set to True to enable display of evaluation results\n",
    "EVAL_DEBUG = False\n",
    "llm_as_judge_model = os.getenv(\"LLM_AS_JUDGE_MODEL_ID\")\n",
    "llm_as_judge_405b_simpleqa_params = llm_as_judge_405b_simpleqa.params.model_copy()\n",
    "# Override the default model\n",
    "# To update the scoring params, we need to provide all the settings, including the defaults\n",
    "llm_as_judge_405b_simpleqa_params.judge_model = llm_as_judge_model\n",
    "\n",
    "# Convert the model dump to a dictionary\n",
    "scoring_params = llm_as_judge_405b_simpleqa_params.model_dump()\n",
    "scoring_params['aggregation_functions']=['categorical_count']\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")\n",
    "print(f\"Eval Parameters:\\n\\tJudge Model: {llm_as_judge_model}\\n\\tQ&A file: {QNA_FILE}\\n\\tMax rows: {MAX_QNA_ROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357655b5-cade-46f4-9f57-be5dcef9abc2",
   "metadata": {},
   "source": [
    "Finally, we will initialize the document collection to be used for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "583421f3-5c77-4964-b525-12f967c20816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered vector DB **test_vector_db_dc9b592d-e12f-4720-8fed-5fedd596ffbd**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "display_markdown(f\"Registered vector DB **{vector_db_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203de51-f570-44ab-8130-36333a54888b",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, conversion and chunking of the documents' content.\n",
    "- Perform a sample query to verify the response is retrieved from the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2dd4664a-ff7f-4474-b6af-3a4ad3f73052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    \"flexible_enhanced_checking/flexible_enhanced_checking.md\",\n",
    "    \"flexible_savings/flexible_savings.md\",\n",
    "    \"flexible_premier_checking/flexible_premier_checking.md\",\n",
    "    \"flexible_core_checking/flexible_core_checking.md\",\n",
    "    \"policies/online_service_agreement.md\",\n",
    "    \"enablement/customer_interactions_resource_guide.md\",\n",
    "    \"enablement/banking_essentials_resource_guide.md\",\n",
    "    \"flexible_money_market_savings/flexible_money_market_savings.md\",\n",
    "    \"flexible_checking/flexible_checking.md\",\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"{url.split('/')[-1]}\",\n",
    "        content=f\"https://raw.githubusercontent.com/jharmison-redhat/parasol-financial-data/main/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a2ee986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flexible_money_market_savings.md',\n",
       " 'flexible_money_market_savings.md',\n",
       " 'flexible_savings.md',\n",
       " 'flexible_savings.md',\n",
       " 'online_service_agreement.md']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query documents\n",
    "results = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=\"What is the Parasol Financial Withdrawal Limit Fee and Transaction Limitations for Flexible Money Market Savings\",\n",
    ")\n",
    "results.metadata['document_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99392db2",
   "metadata": {},
   "source": [
    "## 3. Defining reusable functions\n",
    "Define reusable Python functions to use during the execution of the evaluation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9c3b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_categorical_count(response):\n",
    "    \"\"\"\n",
    "    Computes the evaluation accuracy from the responses of the `llm-as-judge::405b-simpleqa`\n",
    "    scoring function.\n",
    "\n",
    "    Expected responses are:\n",
    "    ```\n",
    "    A: CORRECT\n",
    "    B: INCORRECT\n",
    "    C: NOT_ATTEMPTED\n",
    "    ```\n",
    "    The accuracy is computed as: <number of responses of type `A`> / <number of responses> * 100\n",
    "    \"\"\"\n",
    "    # Evaluate numerical score\n",
    "    correct_answers = sum(\n",
    "        [\n",
    "            count\n",
    "            for cat, count in response.scores[\"llm-as-judge::405b-simpleqa\"]\n",
    "            .aggregated_results[\"categorical_count\"][\"categorical_count\"]\n",
    "            .items()\n",
    "            if cat == \"A\"\n",
    "        ]\n",
    "    )\n",
    "    num_of_scores = len(response.scores[\"llm-as-judge::405b-simpleqa\"].score_rows)\n",
    "    return correct_answers / num_of_scores * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dbb981c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_eval(use_rag: bool):\n",
    "    \"\"\"\n",
    "    Runs the evaluation function for the benchmark indicated by the global variable `qna_benchmark_id`.\n",
    "    A new agent is created for every function call: in case `use_rag` is set to `True`, the `knowledge_search` tool is defined\n",
    "    to implement the RAG workflow.\n",
    "    The global variables `model_id` and `vector_db_id` are also requested.\n",
    "\n",
    "    Params:\n",
    "        use_rag: whether to run a RAG workflow or not.\n",
    "    Returns:\n",
    "        the `Job` associated to the evaluation function.\n",
    "    \"\"\"\n",
    "\n",
    "    from httpx import Timeout\n",
    "\n",
    "    if use_rag == True:\n",
    "        instructions = \"You are a helpful assistant. You must use the knowledge search tool to answer user questions.\"\n",
    "        tools = [\n",
    "            dict(\n",
    "                name=\"builtin::rag\",\n",
    "                args={\n",
    "                    \"vector_db_ids\": [\n",
    "                        vector_db_id\n",
    "                    ],  # list of IDs of document collections to consider during retrieval\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        instructions = \"You are a helpful assistant.\"\n",
    "        tools = []\n",
    "\n",
    "    agent_config = {\n",
    "        \"model\": model_id,\n",
    "        \"instructions\": instructions,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"toolgroups\": tools,\n",
    "    }\n",
    "\n",
    "    _job = client.eval.run_eval(\n",
    "        benchmark_id=qna_benchmark_id,\n",
    "        benchmark_config={\n",
    "            \"num_examples\": MAX_QNA_ROWS,\n",
    "            \"scoring_params\": {\n",
    "                \"llm-as-judge::405b-simpleqa\": scoring_params,\n",
    "            },\n",
    "            \"eval_candidate\": {\n",
    "                \"type\": \"agent\",\n",
    "                \"config\": agent_config,\n",
    "            },\n",
    "        },\n",
    "        timeout=Timeout(MAX_QNA_ROWS * 30),  # Allow for 30s per Q&A\n",
    "    )\n",
    "    return _job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d406ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(job_id, benchmark_id):\n",
    "    return client.eval.jobs.status(job_id=job_id, benchmark_id=benchmark_id).status\n",
    "\n",
    "def _get_eval_reponse(_job):\n",
    "    \"\"\"\n",
    "    Returns the `EvalResponse` instance for the given `_job`.\n",
    "\n",
    "    Params:\n",
    "        `job_id`: The evaluation `Job`.\n",
    "    Returns:\n",
    "        The `EvalResponse` for the given `_job`\n",
    "    \"\"\"\n",
    "    status = get_job_status(\n",
    "        benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "    )\n",
    "    while status != \"completed\":\n",
    "        print(f\"Job status is {status}\")\n",
    "        sleep(1)\n",
    "        status = client.eval.jobs.status(\n",
    "            benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "        ).status\n",
    "    print(f\"Job status is {status}\")\n",
    "    _eval_response = client.eval.jobs.retrieve(\n",
    "        benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "    )\n",
    "\n",
    "    return _eval_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08b987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(score_row):\n",
    "    \"\"\"\n",
    "    Returns the display label for the given `score_row`.\n",
    "    \"\"\"\n",
    "    grades = {'A': 'CORRECT', 'B': 'INCORRECT', 'C': 'NOT_ATTEMPTED'}\n",
    "    score = score_row.get('score', str(score_row))\n",
    "    return grades.get(score,  f'UNKNOWN {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f3c5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_scores(response):\n",
    "    \"\"\"\n",
    "    Converts the computed scores in a numeric array, where scores `A` are evaluated to 1\n",
    "    and all the others to `0`.\n",
    "    \"\"\"\n",
    "    def category_to_number(category):\n",
    "        if category == 'A':\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    return [category_to_number(score_row['score']) for score_row in response.scores['llm-as-judge::405b-simpleqa'].score_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a375726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_for_paired_samples(scores_a, scores_b, iterations=10_000):\n",
    "    \"\"\"\n",
    "    Performs a permutation test of a given statistic on provided data.\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.stats import permutation_test\n",
    "\n",
    "\n",
    "    def _statistic(x, y, axis):\n",
    "        return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "\n",
    "    result = permutation_test(\n",
    "        data=(scores_a, scores_b),\n",
    "        statistic=_statistic,\n",
    "        n_resamples=iterations,\n",
    "        alternative='two-sided',\n",
    "        permutation_type='samples'\n",
    "    )\n",
    "    return float(result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bee78f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats_significance(scores_a, scores_b, label_a, label_b):\n",
    "    mean_score_a = np.mean(scores_a)\n",
    "    mean_score_b = np.mean(scores_b)\n",
    "\n",
    "    p_value = permutation_test_for_paired_samples(scores_a, scores_b)\n",
    "    print(model_id)\n",
    "    print(f\" {label_a:<50}: {mean_score_a:>10.4f}\")\n",
    "    print(f\" {label_b:<50}: {mean_score_b:>10.4f}\")\n",
    "    print(f\" {'p_value':<50}: {p_value:>10.4f}\")\n",
    "    print()\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"p_value<0.05 so this result is statistically significant\")\n",
    "        # Note that the logic below if wrong if the mean scores are equal, but that can't be true if p<1.\n",
    "        higher_model_id = (\n",
    "            label_a\n",
    "            if mean_score_a >= mean_score_b\n",
    "            else label_b\n",
    "        )\n",
    "        print(f\"You can conclude that {higher_model_id} generation is better on data of this sort\")\n",
    "    else:\n",
    "        import math\n",
    "\n",
    "        print(\"p_value>=0.05 so this result is NOT statistically significant.\")\n",
    "        print(\n",
    "            f\"You can conclude that there is not enough data to tell which is better.\"\n",
    "        )\n",
    "        num_samples = len(scores_a)\n",
    "        margin_of_error = 1 / math.sqrt(num_samples)\n",
    "        print(\n",
    "            f\"Note that this data includes {num_samples} questions which typically produces a margin of error of around +/-{margin_of_error:.1%}.\"\n",
    "        )\n",
    "        print(f\"So the two are probably roughly within that margin of error or so.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84253d",
   "metadata": {},
   "source": [
    "## 4. Creating an evaluation Dataset\n",
    "- Load the Q&A file as a Pandas DataFrame.\n",
    "- Transform the dataset to a schema suitable for LLS evaluations.\n",
    "- Register a new Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca041b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(QNA_FILE, \"r\") as f:\n",
    "    qnas_df = pd.read_json(f, lines=True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "145d5514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>chat_completion_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What methods may be used to communicate changes to the Agreement?</td>\n",
       "      <td>Changes may be communicated by mail, email, or a notice on the website.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What methods may be used to communicate changes to the Agreement?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why might it be important to resolve a customer's issue the first time they contact you?</td>\n",
       "      <td>Resolving a customer's issue on the first contact is important because it ensures high-quality service, addresses customer concerns efficiently, and provides the right resolution, which can lead to increased customer satisfaction and loyalty.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"Why might it be important to resolve a customer's issue the first time they contact you?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of fees might you incur if your available balance is not sufficient to process scheduled payments or transfers?</td>\n",
       "      <td>Returned item fees from the payee or overdraft fees.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What type of fees might you incur if your available balance is not sufficient to process scheduled payments or transfers?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the benefits of acting with empathy towards customers?</td>\n",
       "      <td>The benefits of acting with empathy towards customers include gaining their trust, engaging in better conversations, encouraging them to open up about their life, needs, and goals, having them listen to your point of view, being considered the next time they have a need, being referred to others, and agreeing to connect with you again.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What are the benefits of acting with empathy towards customers?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the publisher of the resource guide on financial services?</td>\n",
       "      <td>Parasol Financial Corporation.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"Who is the publisher of the resource guide on financial services?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 input_query  \\\n",
       "0                                                          What methods may be used to communicate changes to the Agreement?   \n",
       "1                                   Why might it be important to resolve a customer's issue the first time they contact you?   \n",
       "2  What type of fees might you incur if your available balance is not sufficient to process scheduled payments or transfers?   \n",
       "3                                                            What are the benefits of acting with empathy towards customers?   \n",
       "4                                                          Who is the publisher of the resource guide on financial services?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                     expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                            Changes may be communicated by mail, email, or a notice on the website.   \n",
       "1                                                                                                 Resolving a customer's issue on the first contact is important because it ensures high-quality service, addresses customer concerns efficiently, and provides the right resolution, which can lead to increased customer satisfaction and loyalty.   \n",
       "2                                                                                                                                                                                                                                                                                               Returned item fees from the payee or overdraft fees.   \n",
       "3  The benefits of acting with empathy towards customers include gaining their trust, engaging in better conversations, encouraging them to open up about their life, needs, and goals, having them listen to your point of view, being considered the next time they have a need, being referred to others, and agreeing to connect with you again.   \n",
       "4                                                                                                                                                                                                                                                                                                                     Parasol Financial Corporation.   \n",
       "\n",
       "                                                                                                                                                         chat_completion_input  \n",
       "0                                                          [{\"role\": \"user\", \"content\": \"What methods may be used to communicate changes to the Agreement?\", \"context\": null}]  \n",
       "1                                   [{\"role\": \"user\", \"content\": \"Why might it be important to resolve a customer's issue the first time they contact you?\", \"context\": null}]  \n",
       "2  [{\"role\": \"user\", \"content\": \"What type of fees might you incur if your available balance is not sufficient to process scheduled payments or transfers?\", \"context\": null}]  \n",
       "3                                                            [{\"role\": \"user\", \"content\": \"What are the benefits of acting with empathy towards customers?\", \"context\": null}]  \n",
       "4                                                          [{\"role\": \"user\", \"content\": \"Who is the publisher of the resource guide on financial services?\", \"context\": null}]  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_stack.apis.inference import UserMessage\n",
    "import json\n",
    "import random\n",
    "\n",
    "qna_dataset_rows = []\n",
    "\n",
    "chat_completion_input = UserMessage(content=\"\")\n",
    "for i in range(len(qnas_df)):\n",
    "    qna = {}\n",
    "    qna[\"input_query\"] = qnas_df.iloc[i][\"question\"]\n",
    "    qna[\"expected_answer\"] = qnas_df.iloc[i][\"answer\"]\n",
    "\n",
    "    chat_completion_input.content = qna[\"input_query\"]\n",
    "    qna[\"chat_completion_input\"] = json.dumps([chat_completion_input.model_dump()])\n",
    "\n",
    "    qna_dataset_rows.append(qna)\n",
    "\n",
    "random.shuffle(qna_dataset_rows)\n",
    "qna_dataset_df = pd.DataFrame(qna_dataset_rows)\n",
    "qna_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22fd5734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered dataset **test_dataset_04b8b2fa-3622-11f0-b2c2-4a70c355aff9**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_dataset_id = f\"test_dataset_{uuid.uuid1()}\"\n",
    "_ = client.datasets.register(\n",
    "    purpose=\"eval/messages-answer\",\n",
    "    source={\n",
    "        \"type\": \"rows\",\n",
    "        \"rows\": qna_dataset_rows,\n",
    "    },\n",
    "    dataset_id=qna_dataset_id,\n",
    ")\n",
    "display_markdown(f\"Registered dataset **{qna_dataset_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726c94",
   "metadata": {},
   "source": [
    "## 5. LLM Eval with reference implementation\n",
    "\n",
    "### 5.1 Registering the evaluation benchmark\n",
    "- Register a Benchmark using the Dataset and the `llm-as-judge::405b-simpleqa` scoring function.\n",
    "- The benchmark is connected to the provider by the `provider_id` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3b669a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered benchmark **test_benchmark_3d97a338-3622-11f0-b2c2-4a70c355aff9**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_benchmark_id = f\"test_benchmark_{uuid.uuid1()}\"\n",
    "client.benchmarks.register(\n",
    "    provider_id=\"meta-reference\",\n",
    "    benchmark_id=qna_benchmark_id,\n",
    "    dataset_id=qna_dataset_id,\n",
    "    scoring_functions=[\"llm-as-judge::405b-simpleqa\"],\n",
    ")\n",
    "display_markdown(f\"Registered benchmark **{qna_benchmark_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd43d5",
   "metadata": {},
   "source": [
    "### 5.2 LLM Eval without RAG\n",
    "- Create an agent configuration without the `knowledge_search` tool.\n",
    "- Run the evaluation function with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "af9ea584",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QNA_ROWS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "451b9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is completed\n",
      "Evaluation of 5 Q&A workflows completed in 30.103 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Computed accuracy is 80.0%**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "without_rag_responses = {}\n",
    "_job = _run_eval(use_rag=False)\n",
    "# pprint(_job)\n",
    "_eval_response = _get_eval_reponse(_job)\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "without_rag_responses[model_id] = _eval_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe7cd9",
   "metadata": {},
   "source": [
    "### 5.3. LLM Eval with RAG\n",
    "- Create an agent configuration with the `knowledge_search` tool.\n",
    "- Run the evaluation function with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "32b7402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is completed\n",
      "Evaluation of 5 Q&A workflows completed in 26.601 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Computed accuracy is 80.0%**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**RAG knowledge search tool used in 4 of (5) agentic calls**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "rag_responses = {}\n",
    "_job = _run_eval(use_rag=True)\n",
    "# pprint(_job)\n",
    "_eval_response = _get_eval_reponse(_job)\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "rag_responses[model_id] = _eval_response\n",
    "\n",
    "retrieved_contexts = sum(\n",
    "    [1 for r in rag_responses[model_id].generations if \"context\" in r]\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**RAG knowledge search tool used in {retrieved_contexts} of ({MAX_QNA_ROWS}) agentic calls**\",\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0d7f9",
   "metadata": {},
   "source": [
    "## 6. LLM Eval with LM-Eval implementation\n",
    "We will run benchmark evaluations on the same dataset via LM-Eval.\n",
    "\n",
    "**Implementation notes**:\n",
    "- The `remote::lmeval` provider is implemented using the [llama_stack_provider_lmeval](https://github.com/trustyai-explainability/llama-stack-provider-lmeval/tree/main/src/llama_stack_provider_lmeval) package.\n",
    "- The provider is connected to the deployed Llama Stack server by the following configuration in the [build.yaml](../../distribution/build.yaml):\n",
    "```yaml\n",
    "...\n",
    "external_providers_dir: ./providers.d\n",
    "```\n",
    "- This `eval` provider delegates the execution of the evaluation function to a separate Kubernetes job that is implemented by the [LMEvalJob](https://trustyai-explainability.github.io/trustyai-site/main/lm-eval-tutorial.html) custom resource. This option provides better scalability and performance to the Llama Stack server, by executing the resource consuming tasks on on-demand\n",
    "jobs dedicated to LLM inference and evaluation.\n",
    "- Finally, the job is implemented as a Custom Task defined in [TrustyAI LM-Eval Tasks](https://github.com/trustyai-explainability/lm-eval-tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22e32a",
   "metadata": {},
   "source": [
    "### 6.1 Data preparation\n",
    "The Q&A document prepared for the reference implementation is not suitable for the LM-Eval task, we need to rename some fields\n",
    "accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08175508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why might it be important to resolve a customer's issue the first time they contact you?</td>\n",
       "      <td>Resolving a customer's issue on the first contact is important because it ensures high-quality service, addresses customer concerns efficiently, and provides the right resolution, which can lead to increased customer satisfaction and loyalty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why might Wealth Management Specialists need to partner with all Parasol partners to deliver a full breadth of solutions?</td>\n",
       "      <td>Partnering with all Parasol partners allows Wealth Management Specialists to offer a comprehensive range of financial solutions, which can help in deepening client relationships and effectively servicing the financial advisor team’s client base.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the steps involved in managing SMS text alerts for the service?</td>\n",
       "      <td>To manage SMS text alerts, you can text STOP to 50014 to stop the alerts. If you want to restore the alerts, you need to go to the Alerts Settings pages and reactivate them. For help, you can send the word HELP to 50014.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What team is included in the additional resources to help employees manage stress?</td>\n",
       "      <td>Life Event Services team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the available payment options for the owner and child of a Core Checking for Family Banking account?</td>\n",
       "      <td>The available payment options are using a Debit Card, Online and Mobile Bill Pay, and Online or Mobile Banking transfers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  user_input  \\\n",
       "0                                   Why might it be important to resolve a customer's issue the first time they contact you?   \n",
       "1  Why might Wealth Management Specialists need to partner with all Parasol partners to deliver a full breadth of solutions?   \n",
       "2                                                   What are the steps involved in managing SMS text alerts for the service?   \n",
       "3                                         What team is included in the additional resources to help employees manage stress?   \n",
       "4              What are the available payment options for the owner and child of a Core Checking for Family Banking account?   \n",
       "\n",
       "                                                                                                                                                                                                                                               reference  \n",
       "0     Resolving a customer's issue on the first contact is important because it ensures high-quality service, addresses customer concerns efficiently, and provides the right resolution, which can lead to increased customer satisfaction and loyalty.  \n",
       "1  Partnering with all Parasol partners allows Wealth Management Specialists to offer a comprehensive range of financial solutions, which can help in deepening client relationships and effectively servicing the financial advisor team’s client base.  \n",
       "2                           To manage SMS text alerts, you can text STOP to 50014 to stop the alerts. If you want to restore the alerts, you need to go to the Alerts Settings pages and reactivate them. For help, you can send the word HELP to 50014.  \n",
       "3                                                                                                                                                                                                                               Life Event Services team  \n",
       "4                                                                                                                              The available payment options are using a Debit Card, Online and Mobile Bill Pay, and Online or Mobile Banking transfers.  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmeval_qna_dataset_df = qna_dataset_df.copy()\n",
    "lmeval_qna_dataset_df.rename(columns={\n",
    "    'input_query': 'user_input',\n",
    "    'expected_answer': 'reference'\n",
    "}, inplace=True)\n",
    "lmeval_qna_dataset_df.drop('chat_completion_input', inplace=True, axis=1)\n",
    "lmeval_qna_dataset_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b497d",
   "metadata": {},
   "source": [
    "We save this to a separate file and copy it in a PVC object that will be mounted to the LM-Eval job. The `dataset-storage-pod` Pod,\n",
    "created by the `lmeval` overlay deployment, is used as the initializer to feed the data into the job.\n",
    "\n",
    "**Note**: we assume the `oc` CLI is available and logged into the target namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "636bf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmeval_qna_dataset_df.to_json(\n",
    "    f\"data/lmeval_qna_dataset.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "!oc cp ./data/lmeval_qna_dataset.jsonl dataset-storage-pod:/data/upload_files/lmeval_qna_dataset.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266eb3d0",
   "metadata": {},
   "source": [
    "### 6.2 Registering the evaluation benchmark\n",
    "The LM-Eval benchmark includes information about the custom task to be executed, the inference environment and the PVC to be mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c0e42194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered benchmark **lmeval::dk-bench**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Important note: This part after the '::' must match the task ID in the task repo\n",
    "lmeval_benchmark_id = \"lmeval::dk-bench\"\n",
    "client.benchmarks.register(\n",
    "    benchmark_id=lmeval_benchmark_id,\n",
    "    dataset_id=lmeval_benchmark_id,\n",
    "    provider_id=\"lmeval\",\n",
    "    scoring_functions=[\"string\"],\n",
    "    provider_benchmark_id=\"string\",\n",
    "    # provide the GH details of the task\n",
    "    metadata={\n",
    "        \"custom_task\": {\n",
    "            \"git\": {\n",
    "                \"url\": \"https://github.com/trustyai-explainability/lm-eval-tasks.git\",\n",
    "                \"branch\": \"main\",\n",
    "                \"commit\": \"8220e2d73c187471acbe71659c98bccecfe77958\",\n",
    "                \"path\": \"tasks/\",\n",
    "            }\n",
    "        },\n",
    "        # provide container environment variables\n",
    "        \"env\": {\n",
    "            # specify path to the DK-Bench dataset within the dataset-storage-pod\n",
    "            \"DK_BENCH_DATASET_PATH\": \"/opt/app-root/src/hf_home/lmeval_qna_dataset.jsonl\",\n",
    "            # specify judge model arguments\n",
    "            \"JUDGE_MODEL_URL\": \"https://api.openai.com/v1\",\n",
    "            \"JUDGE_MODEL_NAME\": llm_as_judge_model,\n",
    "            \"JUDGE_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        },\n",
    "        # specify PVC name of the PVC to be used for the container\n",
    "        \"input\": {\"storage\": {\"pvc\": \"lmeval-data\"}},\n",
    "    },\n",
    ")\n",
    "display_markdown(f\"Registered benchmark **{lmeval_benchmark_id}**\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c3452",
   "metadata": {},
   "source": [
    "### 6.3. LM-Eval without RAG\n",
    "Launch the job from the Llama Stack client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5488a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'lmeval-job-af388259-3991-48a8-9f35-a434b6ed36f0'\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "job = client.eval.run_eval(\n",
    "    benchmark_id=\"lmeval::dk-bench\",\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": model_id,\n",
    "            \"provider_id\": \"lmeval\",\n",
    "            \"sampling_params\": sampling_params,\n",
    "\n",
    "        },\n",
    "        \"num_examples\": MAX_QNA_ROWS\n",
    "    },)\n",
    "\n",
    "print(f\"Starting job '{job.job_id}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0d1d6",
   "metadata": {},
   "source": [
    "Wait until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d2d9a8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ended with status: failed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    status = get_job_status(job_id=job.job_id, benchmark_id=\"lmeval::dk-bench\")\n",
    "\n",
    "    if status in ['failed', 'completed']:\n",
    "        print(f\"Job ended with status: {status}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb7480",
   "metadata": {},
   "source": [
    "Retrieve the result from the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "16c6127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of 5 Q&A workflows completed in 23079.225 seconds\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'llm-as-judge::405b-simpleqa'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m     pprint(_eval_response)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m      9\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_QNA_ROWS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Q&A workflows completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m display_markdown(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m**Computed accuracy is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43maccuracy_from_categorical_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eval_response\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%**\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     raw=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m lmeval_without_rag_responses[model_id] = _eval_response\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36maccuracy_from_categorical_count\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mComputes the evaluation accuracy from the responses of the `llm-as-judge::405b-simpleqa`\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mscoring function.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03mThe accuracy is computed as: <number of responses of type `A`> / <number of responses> * 100\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Evaluate numerical score\u001b[39;00m\n\u001b[32m     15\u001b[39m correct_answers = \u001b[38;5;28msum\u001b[39m(\n\u001b[32m     16\u001b[39m     [\n\u001b[32m     17\u001b[39m         count\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m cat, count \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm-as-judge::405b-simpleqa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m         .aggregated_results[\u001b[33m\"\u001b[39m\u001b[33mcategorical_count\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcategorical_count\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     20\u001b[39m         .items()\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m cat == \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     ]\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m num_of_scores = \u001b[38;5;28mlen\u001b[39m(response.scores[\u001b[33m\"\u001b[39m\u001b[33mllm-as-judge::405b-simpleqa\u001b[39m\u001b[33m\"\u001b[39m].score_rows)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m correct_answers / num_of_scores * \u001b[32m100\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'llm-as-judge::405b-simpleqa'"
     ]
    }
   ],
   "source": [
    "_eval_response = client.eval.jobs.retrieve(\n",
    "    job_id=job.job_id,\n",
    "    benchmark_id=\"lmeval::dk-bench\")\n",
    "\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "lmeval_without_rag_responses[model_id] = _eval_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb3dba",
   "metadata": {},
   "source": [
    "### 6.4. LM-Eval with RAG\n",
    "**TODO**: Add the agent evaluation to LM-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "17163c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmeval_with_rag_responses = {}\n",
    "lmeval_with_rag_responses[model_id] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13682ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 4. Reporting\n",
    "- Aggregated accuracy.\n",
    "- Individual scores and responses.\n",
    "- Statistical Significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4df0d8e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lmeval_without_rag_responses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[185]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m pd_responses[\u001b[33m'\u001b[39m\u001b[33mexpected\u001b[39m\u001b[33m'\u001b[39m] = [qna_dataset_rows[i][\u001b[33m'\u001b[39m\u001b[33mexpected_answer\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_QNA_ROWS)]\n\u001b[32m      5\u001b[39m pd_accuracies = {}\n\u001b[32m      6\u001b[39m df_accuracies = pd.DataFrame.from_dict({\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: without_rag_responses.keys(),\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m(reference) Accuracy without RAG\u001b[39m\u001b[33m'\u001b[39m: [accuracy_from_categorical_count(without_rag_responses[model_id]) \u001b[38;5;28;01mfor\u001b[39;00m model_id \u001b[38;5;129;01min\u001b[39;00m without_rag_responses.keys()],\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m(reference) Accuracy with RAG\u001b[39m\u001b[33m'\u001b[39m: [accuracy_from_categorical_count(rag_responses[model_id]) \u001b[38;5;28;01mfor\u001b[39;00m model_id \u001b[38;5;129;01min\u001b[39;00m rag_responses.keys()],\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m(lmeval) Accuracy without RAG\u001b[39m\u001b[33m'\u001b[39m: [accuracy_from_categorical_count(\u001b[43mlmeval_without_rag_responses\u001b[49m[model_id]) \u001b[38;5;28;01mfor\u001b[39;00m model_id \u001b[38;5;129;01min\u001b[39;00m without_rag_responses.keys()],\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m(lmeval) Accuracy with RAG\u001b[39m\u001b[33m'\u001b[39m: [accuracy_from_categorical_count(lmeval_rag_responses[model_id]) \u001b[38;5;28;01mfor\u001b[39;00m model_id \u001b[38;5;129;01min\u001b[39;00m rag_responses.keys()]})\n\u001b[32m     12\u001b[39m df_accuracies.style.hide()\n",
      "\u001b[31mNameError\u001b[39m: name 'lmeval_without_rag_responses' is not defined"
     ]
    }
   ],
   "source": [
    "pd_responses = {}\n",
    "pd_responses['questions'] = [qna_dataset_rows[i]['input_query'] for i in range(MAX_QNA_ROWS)]\n",
    "pd_responses['expected'] = [qna_dataset_rows[i]['expected_answer'] for i in range(MAX_QNA_ROWS)]\n",
    "\n",
    "pd_accuracies = {}\n",
    "df_accuracies = pd.DataFrame.from_dict({\n",
    "    'Model': without_rag_responses.keys(),\n",
    "    '(reference) Accuracy without RAG': [accuracy_from_categorical_count(without_rag_responses[model_id]) for model_id in without_rag_responses.keys()],\n",
    "    '(reference) Accuracy with RAG': [accuracy_from_categorical_count(rag_responses[model_id]) for model_id in rag_responses.keys()],\n",
    "    '(lmeval) Accuracy without RAG': [accuracy_from_categorical_count(lmeval_without_rag_responses[model_id]) for model_id in without_rag_responses.keys()],\n",
    "    '(lmeval) Accuracy with RAG': [accuracy_from_categorical_count(lmeval_rag_responses[model_id]) for model_id in rag_responses.keys()]})\n",
    "df_accuracies.style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "95b9baa2-4739-426a-b79a-2ff90f44c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_data = {}\n",
    "ratings_data = {}\n",
    "responses_data = {}\n",
    "\n",
    "report_data['Question'] = [qna_dataset_rows[i]['input_query'] for i in range(MAX_QNA_ROWS)]\n",
    "ratings_data['Question'] = report_data['Question']\n",
    "responses_data['Question'] = report_data['Question']\n",
    "report_data['Expected Answer'] = [qna_dataset_rows[i]['expected_answer'] for i in range(MAX_QNA_ROWS)]\n",
    "responses_data['Expected Answer'] = report_data['Expected Answer']\n",
    "for model_id in without_rag_responses.keys():\n",
    "    report_data[f'Rating without RAG'] = [to_label(score_row) for score_row in without_rag_responses[model_id].scores['llm-as-judge::405b-simpleqa'].score_rows]\n",
    "    report_data[f'Answer without RAG'] = [g['generated_answer'] for g in without_rag_responses[model_id].generations]\n",
    "    report_data[f'Rating with RAG'] = [to_label(score_row) for score_row in rag_responses[model_id].scores['llm-as-judge::405b-simpleqa'].score_rows]\n",
    "    report_data[f'Answer with RAG'] = [g['generated_answer'] for g in rag_responses[model_id].generations]\n",
    "    \n",
    "    ratings_data[f'Rating without RAG'] = report_data[f'Rating without RAG']\n",
    "    responses_data[f'Answer without RAG'] = report_data[f'Answer without RAG']\n",
    "    ratings_data[f'with RAG RAG Rating'] = report_data[f'Rating with RAG']\n",
    "    responses_data[f'Answer with RAG'] = report_data[f'Answer with RAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "272729b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response Comparison for model granite32-8b**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question</th>\n",
       "      <th>Expected Answer</th>\n",
       "      <th>Rating without RAG</th>\n",
       "      <th>Answer without RAG</th>\n",
       "      <th>Rating with RAG</th>\n",
       "      <th>Answer with RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>What methods may be used to communicate changes to the Agreement?</td>\n",
       "      <td>Changes may be communicated by mail, email, or a notice on the website.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>1. Email Notifications: This is a common method where changes to the agreement are communicated via email. It's important to clearly state the nature of the changes and their effective date.\\n\\n2. In-App Notifications: If the agreement is digital and used within an application, in-app notifications can be an effective way to communicate changes.\\n\\n3. Postal Mail: For formal agreements, changes might be communicated via postal mail, especially if the agreement is legally binding and requires a physical signature for acceptance.\\n\\n4. Website Announcements: If the agreement is available online, changes can be posted on the website with a clear notice about the revision date.\\n\\n5. Telephone or Video Call: For critical changes, a direct communication method like a phone call or video conference might be used to ensure the recipient understands the changes.\\n\\n6. Physical Copies: For agreements that are in physical form, changes can be communicated by sending out revised copies.\\n\\n7. Legal Notices: For significant changes, legal notices might be published in newspapers or on public notice boards.\\n\\n8. Agreement Update Platforms: Some platforms allow for direct updates within the agreement document itself, with a clear record of changes and who made them.\\n\\n9. Training Sessions: For complex changes, training sessions or webinars might be organized to explain the changes and their implications.\\n\\n10. Acknowledgment Forms: After communicating the changes, an acknowledgment form might be sent to the recipient to confirm they have read, understood, and agreed to the new terms. \\n\\nRemember, it's crucial to ensure that the method of communication is accessible and understandable to all parties involved, and that there's a clear record of when and how the changes were communicated.</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>Based on the retrieved information, changes to the agreement can be communicated through the following methods:\\n\\n1. **Mail**: The document mentions that changes to the agreement may be communicated via mail.\\n\\n2. **Email**: Changes can also be communicated through email. This is a common method for businesses to notify customers of updates or changes to their terms of service or agreements.\\n\\n3. **Notice on the website**: The agreement states that changes will be posted on the company's website. This is a digital method that allows for immediate access to updates for users who regularly check the site.\\n\\n4. **Secure mailbox**: This is a more secure method of communication, possibly used for sensitive information.\\n\\nRemember, the agreement also mentions that by continuing to use the services after the date that changes are posted to the website, those changes will be effective for transactions made after that date. This implies that regularly checking the website for updates is a crucial method for staying informed about changes to the agreement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Why might it be important to resolve a customer's issue the first time they contact you?</td>\n",
       "      <td>Resolving a customer's issue on the first contact is important because it ensures high-quality service, addresses customer concerns efficiently, and provides the right resolution, which can lead to increased customer satisfaction and loyalty.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Resolving a customer's issue the first time they contact you is crucial for several reasons:\\n\\n1. **Customer Satisfaction**: Customers appreciate quick and efficient service. If their issue is resolved promptly, they're likely to feel satisfied and valued, which can enhance their overall experience with your brand.\\n\\n2. **Reduced Effort**: Each subsequent contact a customer has to make to resolve an issue increases their effort. This can lead to frustration and dissatisfaction. By solving it the first time, you're minimizing their effort and potential annoyance.\\n\\n3. **Cost Efficiency**: Each additional interaction with a customer service team costs more. This includes not just the time of the agent, but also the resources used (like phone lines, email servers, etc.). First-contact resolution (FCR) can significantly reduce these costs.\\n\\n4. **Loyalty and Retention**: Satisfied customers are more likely to remain loyal to your brand. High FCR rates can contribute to customer retention, which is generally more cost-effective than acquiring new customers.\\n\\n5. **Reputation**: Positive word-of-mouth and online reviews can boost your brand's reputation. Conversely, poor customer service experiences can lead to negative reviews and damage your brand's image.\\n\\n6. **Data and Insights**: Each interaction provides valuable data about your products, services, and customer needs. By resolving issues quickly, you can gather and act on this data more effectively, improving your offerings and service over time.\\n\\n7. **Preventing Escalation**: Unresolved issues can escalate, leading to more complex problems and potentially damaging relationships. First-time resolution helps prevent these escalations.\\n\\nIn summary, first-time resolution is key to enhancing customer satisfaction, reducing costs, fostering loyalty, and improving your overall business performance.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Resolving a customer's issue the first time they contact you is important for several reasons:\\n\\n1. **Customer Satisfaction**: Customers appreciate quick and efficient service. If their issue is resolved immediately, they are likely to feel satisfied with the service they received.\\n\\n2. **Reduced Effort**: For the customer, having to repeat their issue multiple times can be frustrating and time-consuming. Resolving it the first time minimizes their effort.\\n\\n3. **Cost Savings**: Each interaction with a customer service representative costs a company. By resolving issues on the first contact, businesses can save on operational costs.\\n\\n4. **Loyalty and Retention**: Positive experiences with customer service can lead to customer loyalty and retention. Satisfied customers are more likely to continue doing business with a company.\\n\\n5. **Preventing Escalation**: If an issue isn't resolved, it might escalate, potentially leading to more serious problems or even public complaints.\\n\\n6. **Data Accuracy**: Each time a customer's issue is relayed, there's a chance for miscommunication or misinterpretation. Resolving it the first time ensures data accuracy.\\n\\nThese points are based on general customer service principles and best practices. For more specific or detailed information, a knowledge search could be conducted using terms like \"first contact resolution benefits\" or \"customer service metrics\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What type of fees might you incur if your available balance is not sufficient to process scheduled payments or transfers?</td>\n",
       "      <td>Returned item fees from the payee or overdraft fees.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>If your available balance is insufficient to process scheduled payments or transfers, you may incur several types of fees, depending on your bank or financial institution:\\n\\n1. **Insufficient Funds Fee**: This is a common fee charged when there aren't enough funds in your account to cover a transaction. The exact amount can vary, but it's typically a flat fee.\\n\\n2. **Overdraft Fee**: If your account is set up to allow overdrafts (where the bank covers the transaction and then charges you for it), you might be charged an overdraft fee. This can be a flat fee or a variable fee based on the amount overdrawn.\\n\\n3. **Returned Item Fee**: If a payment or transfer is declined due to insufficient funds and then attempted again, each attempt could result in a returned item fee.\\n\\n4. **NSF (Non-Sufficient Funds) Fee**: This is another term for an insufficient funds fee. It's charged when there's not enough money in your account to cover a transaction.\\n\\n5. **Late Payment Fee**: If the insufficient funds cause a payment to be late, you might also be charged a late payment fee.\\n\\n6. **Reconnection Fee**: If the insufficient funds result in a utility service being disconnected, you'll likely have to pay a reconnection fee once the service is restored.\\n\\nRemember, it's always a good idea to keep track of your account balance to avoid these fees. Many banks offer overdraft protection services or linked savings accounts that can help prevent these charges. Always review your bank's fee schedule to understand what charges you might face in such situations.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Based on the information retrieved, if your available balance is not sufficient to process scheduled payments or transfers, you may incur an \"Overdraft Item Fee.\" This fee is $10.00 per item, but it won't be charged more than twice per day. \\n\\nThe overdraft fee applies for overdrafts created by checks, recurring debit card transactions, or other electronic means. If your account is overdrawn, you must bring it to a positive balance immediately. The bank reserves the right not to pay overdrafts, for example, if your account is not in good standing.\\n\\nAdditionally, if a payment is initiated and your account does not have sufficient funds when the payment posts, you may incur late fees, interest charges, or other actions taken by the Payee. \\n\\nPlease note that these fees and policies can vary depending on the specific bank or financial institution. It's always a good idea to check with your bank for the most accurate and up-to-date information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What are the benefits of acting with empathy towards customers?</td>\n",
       "      <td>The benefits of acting with empathy towards customers include gaining their trust, engaging in better conversations, encouraging them to open up about their life, needs, and goals, having them listen to your point of view, being considered the next time they have a need, being referred to others, and agreeing to connect with you again.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Acting with empathy towards customers offers several significant benefits:\\n\\n1. **Enhanced Customer Satisfaction**: Empathy helps in understanding the customer's needs, concerns, and feelings better. This understanding can lead to more personalized and effective solutions, thereby increasing customer satisfaction.\\n\\n2. **Improved Customer Loyalty**: When customers feel understood and valued, they are more likely to remain loyal to a brand. Empathetic interactions can foster a sense of connection and trust, leading to repeat business and positive word-of-mouth referrals.\\n\\n3. **Conflict Resolution**: Empathy is crucial in resolving conflicts. By putting oneself in the customer's shoes, one can address issues more effectively and find mutually agreeable solutions, reducing the likelihood of escalations.\\n\\n4. **Increased Sales**: Empathetic customer service can lead to upselling and cross-selling opportunities. By understanding a customer's needs and preferences, service representatives can suggest products or services that genuinely meet those needs, increasing sales.\\n\\n5. **Positive Brand Image**: Consistently empathetic interactions contribute to a positive brand image. This can attract new customers and improve the brand's reputation in the market.\\n\\n6. **Employee Satisfaction**: Employees who are trained to be empathetic often find their work more fulfilling. This can lead to higher job satisfaction, lower turnover rates, and better overall performance.\\n\\n7. **Better Customer Insights**: Empathetic listening can provide valuable insights into customer behavior, preferences, and pain points. These insights can guide product development, marketing strategies, and overall business decisions.\\n\\n8. **Reduced Customer Churn**: By addressing customer concerns with empathy, businesses can reduce the number of customers who stop doing business with them, thereby lowering customer churn rates.\\n\\nIn summary, empathy in customer interactions can lead to improved customer relationships, increased sales, better brand image, and a more positive work environment.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Acting with empathy towards customers can yield several benefits:\\n\\n1. **Trust and Credibility**: When customers feel understood, they are more likely to trust the service provider. Empathy helps in building a strong relationship, which is crucial for customer loyalty.\\n\\n2. **Improved Customer Satisfaction**: Empathetic interactions can lead to better customer experiences. Customers feel valued and heard, which can increase their satisfaction levels.\\n\\n3. **Enhanced Problem-Solving**: Empathy allows service providers to understand the customer's perspective better, leading to more effective problem-solving. It helps in identifying the root cause of the issue and providing tailored solutions.\\n\\n4. **Increased Customer Retention**: Empathetic customer service can lead to higher customer retention rates. When customers feel understood and cared for, they are more likely to continue doing business with the company.\\n\\n5. **Positive Word-of-Mouth**: Empathetic interactions often lead to positive customer experiences, which can result in positive word-of-mouth. Satisfied customers are more likely to recommend the business to others.\\n\\n6. **Better Customer Engagement**: Empathy encourages customers to open up about their needs and goals, leading to more meaningful conversations and a deeper understanding of their requirements.\\n\\n7. **Reduced Customer Complaints**: By showing empathy, service providers can often defuse tense situations before they escalate into formal complaints.\\n\\n8. **Increased Sales**: Empathetic customer service can lead to increased sales. When customers feel understood and valued, they are more likely to make a purchase or consider additional products/services.\\n\\nIn summary, empathy in customer service can lead to more trusting relationships, higher customer satisfaction, better problem-solving, increased retention, positive word-of-mouth, better engagement, fewer complaints, and potentially higher sales.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Who is the publisher of the resource guide on financial services?</td>\n",
       "      <td>Parasol Financial Corporation.</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>Without a specific title or resource guide mentioned, I'm unable to provide an exact publisher. However, many financial services resource guides are published by financial institutions, industry associations, or publishing companies specializing in finance and business. Examples include McGraw-Hill, Wiley, PwC, Deloitte, or specific financial institutions like JPMorgan Chase or Goldman Sachs. Please provide more details if you're referring to a specific guide.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>The publisher of the resource guide on financial services is Parasol Financial Corporation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "display_markdown(f\"**Response Comparison for model {model_id}**\", raw=True)\n",
    "report_df = pd.DataFrame.from_dict(report_data)\n",
    "HTML(report_df.head().to_html(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5409d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Statistical Significance (without Vs with RAG generations)**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granite32-8b\n",
      " accuracy without RAG                              :     0.8000\n",
      " accuracy with RAG                                 :     0.8000\n",
      " p_value                                           :     1.0000\n",
      "\n",
      "p_value>=0.05 so this result is NOT statistically significant.\n",
      "You can conclude that there is not enough data to tell which is better.\n",
      "Note that this data includes 5 questions which typically produces a margin of error of around +/-44.7%.\n",
      "So the two are probably roughly within that margin of error or so.\n"
     ]
    }
   ],
   "source": [
    "display_markdown(\"**Statistical Significance (without Vs with RAG generations)**\", raw= True)\n",
    "print_stats_significance(numeric_scores(without_rag_responses[model_id]), numeric_scores(rag_responses[model_id]), \"accuracy without RAG\", \"accuracy with RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This tutorial demonstrates how to evaluate an agentic workflow, without and without RAG tool, using the Llama Stack reference implementation.\n",
    "We do so by initializing an agent, with optional access to the RAG tool, then invoking the agent evaluation against a predefined reference of sample Q&A. \n",
    "Please check out our [complementary tutorial](../rag_agentic/notebooks/Level4_RAG_agent.ipynb) for an agentic RAG example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
