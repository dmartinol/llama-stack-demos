{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 3: Agentic RAG with reference Eval\n",
    "\n",
    "This tutorial presents an example of evaluating an agentic RAG in LLama-Stack using the reference implementation. \n",
    "Please refer to the [Level 3: Agentic RAG](./Level3_agentic_RAG.ipynb) notebook for details on how to initialize the agent and the knowledge search RAG tool provided by Llama Stack.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Connecting to a llama-stack server.\n",
    "2. Indexing a collection of documents in a vector DB for later retrieval.\n",
    "3. Initializing the agent capable of retrieving content from vector DB via tool use.\n",
    "4. Evaluating the agent responses against a reference set of Q&A.\n",
    "5. Reporting the evaluation results and its statistical relevance.\n",
    "\n",
    "## Case study\n",
    "For the purpose of this training, we are going to use the fictional company \n",
    "[Parasol Financial](https://www.redhat.com/en/blog/ai-insurance-industry-insights-red-hat-summit-2024), and the provided\n",
    "[training documents](https://github.com/jharmison-redhat/parasol-financial-data/).\n",
    "\n",
    "A sample Q&A document is available as a [reference](./data/parasol-financial-data_qac.yaml). \n",
    "This predefined question and answer pairs have beeen generated using [docling-sdg](https://github.com/docling-project/docling-sdg),\n",
    "an IBM set of tools to create artificial data from documents, leveraging generative AI and Docling's parsing capabilities.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have a running instance of the Llama Stack server (local or remote) with at least one preconfigured vector DB. For more information, please refer to the corresponding [Llama Stack tutorials](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).\n",
    "\n",
    "The `openai` inference provider is required if you intend to use an OpenAI model for judging purposes, like `openai/gpt-4o`. In this case, the \n",
    "`OPENAI_API_KEY` env variable must be configured into the Llama Stack server.\n",
    "\n",
    "## Setting the Environment Variables\n",
    "\n",
    "Use the [`.env.example`](../../../.env.example) to create a new file called `.env` and ensure you add all the relevant environment variables below.\n",
    "\n",
    "In addition to the environment variables listed in the [\"Getting Started\" notebook](demos/rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb), the following should be provided for this demo to run:\n",
    " - `LLM_AS_JUDGE_MODEL_ID`: the model to use as the judge to evaluate the agent responses. Must be one of the models defined in Llama Stack.\n",
    " - `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    " - `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    " - `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    " - `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "We will start with a few imports needed for this demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854e7cb4-aed9-4098-adc1-a66f4c9e6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "from llama_stack_client import Agent, AgentEventLogger, RAGDocument\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab4244-e7af-405b-b0c3-4bf00411f26e",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](demos/rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b87b139-bd18-47b2-889a-1b8ed3018655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n",
      "Inference Parameters:\n",
      "\tModel: granite32-8b\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n",
      "Eval Parameters:\n",
      "\tJudge Model: openai/gpt-4o\n",
      "\tQ&A file: ./data/parasol-financial-data_qac.yaml\n",
      "\tMax rows: 50\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "# to override the judge model\n",
    "from llama_stack.providers.inline.scoring.llm_as_judge.scoring_fn.fn_defs.llm_as_judge_405b_simpleqa import (\n",
    "    llm_as_judge_405b_simpleqa,\n",
    ")\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "remote = os.getenv(\"REMOTE\", \"True\")\n",
    "\n",
    "if remote == \"False\":\n",
    "    local_port = os.getenv(\"LOCAL_SERVER_PORT\", 8321)\n",
    "    base_url = f\"http://localhost:{local_port}\"\n",
    "else: # any value non equal to 'False' will be considered as 'True'\n",
    "    base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=None\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server @ {base_url}\")\n",
    "\n",
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "# The Q&A file\n",
    "QNA_FILE = './data/parasol-financial-data_qac.yaml'\n",
    "# The number of rows to consider\n",
    "MAX_QNA_ROWS = 50\n",
    "# Set to True to enable display of evaluation results\n",
    "EVAL_DEBUG = False\n",
    "llm_as_judge_model = os.getenv(\"LLM_AS_JUDGE_MODEL_ID\")\n",
    "llm_as_judge_405b_simpleqa_params = llm_as_judge_405b_simpleqa.params.model_copy()\n",
    "# Override the default model\n",
    "# To update the scoring params, we need to provide all the settings, including the defaults\n",
    "llm_as_judge_405b_simpleqa_params.judge_model = llm_as_judge_model\n",
    "\n",
    "# Convert the model dump to a dictionary\n",
    "scoring_params = llm_as_judge_405b_simpleqa_params.model_dump()\n",
    "scoring_params['aggregation_functions']=['categorical_count']\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")\n",
    "print(f\"Eval Parameters:\\n\\tJudge Model: {llm_as_judge_model}\\n\\tQ&A file: {QNA_FILE}\\n\\tMax rows: {MAX_QNA_ROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357655b5-cade-46f4-9f57-be5dcef9abc2",
   "metadata": {},
   "source": [
    "Finally, we will initialize the document collection to be used for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583421f3-5c77-4964-b525-12f967c20816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered vector DB **test_vector_db_92f5fff6-4685-4cdf-a98f-ad3a84cda07c**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "display_markdown(f\"Registered vector DB **{vector_db_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203de51-f570-44ab-8130-36333a54888b",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, conversion and chunking of the documents' content.\n",
    "- Perform a sample query to verify the response is retrieved from the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd4664a-ff7f-4474-b6af-3a4ad3f73052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    \"flexible_enhanced_checking/flexible_enhanced_checking.md\",\n",
    "    \"flexible_savings/flexible_savings.md\",\n",
    "    \"flexible_premier_checking/flexible_premier_checking.md\",\n",
    "    \"flexible_core_checking/flexible_core_checking.md\",\n",
    "    \"policies/online_service_agreement.md\",\n",
    "    \"enablement/customer_interactions_resource_guide.md\",\n",
    "    \"enablement/banking_essentials_resource_guide.md\",\n",
    "    \"flexible_money_market_savings/flexible_money_market_savings.md\",\n",
    "    \"flexible_checking/flexible_checking.md\",\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"{url.split('/')[-1]}\",\n",
    "        content=f\"https://raw.githubusercontent.com/jharmison-redhat/parasol-financial-data/main/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2ee986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flexible_money_market_savings.md',\n",
       " 'flexible_money_market_savings.md',\n",
       " 'flexible_savings.md',\n",
       " 'flexible_savings.md',\n",
       " 'online_service_agreement.md']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query documents\n",
    "results = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=\"What is the Parasol Financial Withdrawal Limit Fee and Transaction Limitations for Flexible Money Market Savings\",\n",
    ")\n",
    "results.metadata['document_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99392db2",
   "metadata": {},
   "source": [
    "## 3. Defining reusable functions\n",
    "Define reusable Python functions to use during the execution of the evaluation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c3b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_categorical_count(response):\n",
    "    \"\"\"\n",
    "    Computes the evaluation accuracy from the responses of the `llm-as-judge::405b-simpleqa`\n",
    "    scoring function.\n",
    "\n",
    "    Expected responses are:\n",
    "    ```\n",
    "    A: CORRECT\n",
    "    B: INCORRECT\n",
    "    C: NOT_ATTEMPTED\n",
    "    ```\n",
    "    The accuracy is computed as: <number of responses of type `A`> / <number of responses> * 100\n",
    "    \"\"\"\n",
    "    # Evaluate numerical score\n",
    "    correct_answers = sum(\n",
    "        [\n",
    "            count\n",
    "            for cat, count in response.scores[\"llm-as-judge::405b-simpleqa\"]\n",
    "            .aggregated_results[\"categorical_count\"][\"categorical_count\"]\n",
    "            .items()\n",
    "            if cat == \"A\"\n",
    "        ]\n",
    "    )\n",
    "    num_of_scores = len(response.scores[\"llm-as-judge::405b-simpleqa\"].score_rows)\n",
    "    return correct_answers / num_of_scores * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb981c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_eval(use_rag: bool):\n",
    "    \"\"\"\n",
    "    Runs the evaluation function for the benchmark indicated by the global variable `qna_benchmark_id`.\n",
    "    A new agent is created for every function call: in case `use_rag` is set to `True`, the `knowledge_search` tool is defined\n",
    "    to implement the RAG workflow.\n",
    "    The global variables `model_id` and `vector_db_id` are also requested.\n",
    "\n",
    "    Params:\n",
    "        use_rag: whether to run a RAG workflow or not.\n",
    "    Returns:\n",
    "        the `Job` associated to the evaluation function.\n",
    "    \"\"\"\n",
    "\n",
    "    from httpx import Timeout\n",
    "\n",
    "    if use_rag == True:\n",
    "        instructions = \"You are a helpful assistant. You must use the knowledge search tool to answer user questions.\"\n",
    "        tools = [\n",
    "            dict(\n",
    "                name=\"builtin::rag\",\n",
    "                args={\n",
    "                    \"vector_db_ids\": [\n",
    "                        vector_db_id\n",
    "                    ],  # list of IDs of document collections to consider during retrieval\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        instructions = \"You are a helpful assistant.\"\n",
    "        tools = []\n",
    "\n",
    "    agent_config = {\n",
    "        \"model\": model_id,\n",
    "        \"instructions\": instructions,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"toolgroups\": tools,\n",
    "    }\n",
    "\n",
    "    _job = client.eval.run_eval(\n",
    "        benchmark_id=qna_benchmark_id,\n",
    "        benchmark_config={\n",
    "            \"num_examples\": MAX_QNA_ROWS,\n",
    "            \"scoring_params\": {\n",
    "                \"llm-as-judge::405b-simpleqa\": scoring_params,\n",
    "            },\n",
    "            \"eval_candidate\": {\n",
    "                \"type\": \"agent\",\n",
    "                \"config\": agent_config,\n",
    "            },\n",
    "        },\n",
    "        timeout=Timeout(MAX_QNA_ROWS * 30),  # Allow for 30s per Q&A\n",
    "    )\n",
    "    return _job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d406ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_eval_reponse(_job):\n",
    "    \"\"\"\n",
    "    Returns the `EvalResponse` instance for the given `_job`.\n",
    "\n",
    "    Params:\n",
    "        `job_id`: The evaluation `Job`.\n",
    "    Returns:\n",
    "        The `EvalResponse` for the given `_job`\n",
    "    \"\"\"\n",
    "    status = client.eval.jobs.status(\n",
    "        benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "    ).status\n",
    "    while status != \"completed\":\n",
    "        print(f\"Job status is {status}\")\n",
    "        sleep(1)\n",
    "        status = client.eval.jobs.status(\n",
    "            benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "        ).status\n",
    "    print(f\"Job status is {status}\")\n",
    "    _eval_response = client.eval.jobs.retrieve(\n",
    "        benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "    )\n",
    "\n",
    "    return _eval_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(score_row):\n",
    "    \"\"\"\n",
    "    Returns the display label for the given `score_row`.\n",
    "    \"\"\"\n",
    "    grades = {'A': 'CORRECT', 'B': 'INCORRECT', 'C': 'NOT_ATTEMPTED'}\n",
    "    score = score_row.get('score', str(score_row))\n",
    "    return grades.get(score,  f'UNKNOWN {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3c5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_scores(response):\n",
    "    \"\"\"\n",
    "    Converts the computed scores in a numeric array, where scores `A` are evaluated to 1\n",
    "    and all the others to `0`.\n",
    "    \"\"\"\n",
    "    def category_to_number(category):\n",
    "        if category == 'A':\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    return [category_to_number(score_row['score']) for score_row in response.scores['llm-as-judge::405b-simpleqa'].score_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a375726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_for_paired_samples(scores_a, scores_b, iterations=10_000):\n",
    "    \"\"\"\n",
    "    Performs a permutation test of a given statistic on provided data.\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.stats import permutation_test\n",
    "\n",
    "\n",
    "    def _statistic(x, y, axis):\n",
    "        return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "\n",
    "    result = permutation_test(\n",
    "        data=(scores_a, scores_b),\n",
    "        statistic=_statistic,\n",
    "        n_resamples=iterations,\n",
    "        alternative='two-sided',\n",
    "        permutation_type='samples'\n",
    "    )\n",
    "    return float(result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee78f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats_significance(scores_a, scores_b):\n",
    "    mean_score_a = np.mean(scores_a)\n",
    "    mean_score_b = np.mean(scores_b)\n",
    "\n",
    "    p_value = permutation_test_for_paired_samples(scores_a, scores_b)\n",
    "\n",
    "    print(f\" {model_id:<50}: {mean_score_a:>10.4f}\")\n",
    "    print(f\" {'p_value':<50}: {p_value:>10.4f}\")\n",
    "    print()\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"p_value<0.05 so this result is statistically significant\")\n",
    "        # Note that the logic below if wrong if the mean scores are equal, but that can't be true if p<1.\n",
    "        higher_model_id = (\n",
    "            'vanilla'\n",
    "            if mean_score_a >= mean_score_b\n",
    "            else 'RAG'\n",
    "        )\n",
    "        print(f\"You can conclude that {higher_model_id} generation is better on data of this sort\")\n",
    "    else:\n",
    "        import math\n",
    "\n",
    "        print(\"p_value>=0.05 so this result is NOT statistically significant\")\n",
    "        print(\n",
    "            f\"You can conclude that there is not enough data to tell which is better.\"\n",
    "        )\n",
    "        num_samples = len(scores_a)\n",
    "        margin_of_error = 1 / math.sqrt(num_samples)\n",
    "        print(\n",
    "            f\"Note that this data includes {num_samples} questions which typically produces a margin of error of around +/-{margin_of_error:.1%}.\"\n",
    "        )\n",
    "        print(f\"So the two are probably roughly within that margin of error or so.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84253d",
   "metadata": {},
   "source": [
    "## 4. Creating an evaluation Dataset\n",
    "- Load the Q&A file as a Pandas DataFrame.\n",
    "- Transform the dataset to a schema suitable for LLS evaluations.\n",
    "- Register a new Dataset.\n",
    "- Register a Benchmark using the Dataset and the `llm-as-judge::405b-simpleqa` scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca041b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(QNA_FILE, \"r\") as f:\n",
    "    qnas_df = pd.read_json(f, lines=True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145d5514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>chat_completion_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why might it be beneficial to keep a document with all your answers to the journaling activities?</td>\n",
       "      <td>Keeping a document with all your answers allows you to track your learning and progress, providing a reference point that can help guide your career decisions and development in the financial services industry.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"Why might it be beneficial to keep a document with all your answers to the journaling activities?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the name of the training program implemented by Wealth Management Specialists?</td>\n",
       "      <td>Elite Growth Practice (EGP)</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What is the name of the training program implemented by Wealth Management Specialists?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should you text to opt out of all security alerts?</td>\n",
       "      <td>You should text STOP to any of the short codes to opt out of all security alerts.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What should you text to opt out of all security alerts?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why might Parasol Financial include a quiz on the Better Money Habits homepage?</td>\n",
       "      <td>Parasol Financial might include a quiz on the Better Money Habits homepage to tailor the content to the user's specific life stage and interests, thereby making the financial advice more relevant and effective for each individual.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"Why might Parasol Financial include a quiz on the Better Money Habits homepage?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is one of the job expectations for Senior Bankers?</td>\n",
       "      <td>Proactively connecting with clients through outbound calls and conducting consistent follow-up routines.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What is one of the job expectations for Senior Bankers?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         input_query  \\\n",
       "0  Why might it be beneficial to keep a document with all your answers to the journaling activities?   \n",
       "1             What is the name of the training program implemented by Wealth Management Specialists?   \n",
       "2                                            What should you text to opt out of all security alerts?   \n",
       "3                    Why might Parasol Financial include a quiz on the Better Money Habits homepage?   \n",
       "4                                            What is one of the job expectations for Senior Bankers?   \n",
       "\n",
       "                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                      Keeping a document with all your answers allows you to track your learning and progress, providing a reference point that can help guide your career decisions and development in the financial services industry.   \n",
       "1                                                                                                                                                                                                             Elite Growth Practice (EGP)   \n",
       "2                                                                                                                                                       You should text STOP to any of the short codes to opt out of all security alerts.   \n",
       "3  Parasol Financial might include a quiz on the Better Money Habits homepage to tailor the content to the user's specific life stage and interests, thereby making the financial advice more relevant and effective for each individual.   \n",
       "4                                                                                                                                Proactively connecting with clients through outbound calls and conducting consistent follow-up routines.   \n",
       "\n",
       "                                                                                                                                 chat_completion_input  \n",
       "0  [{\"role\": \"user\", \"content\": \"Why might it be beneficial to keep a document with all your answers to the journaling activities?\", \"context\": null}]  \n",
       "1             [{\"role\": \"user\", \"content\": \"What is the name of the training program implemented by Wealth Management Specialists?\", \"context\": null}]  \n",
       "2                                            [{\"role\": \"user\", \"content\": \"What should you text to opt out of all security alerts?\", \"context\": null}]  \n",
       "3                    [{\"role\": \"user\", \"content\": \"Why might Parasol Financial include a quiz on the Better Money Habits homepage?\", \"context\": null}]  \n",
       "4                                            [{\"role\": \"user\", \"content\": \"What is one of the job expectations for Senior Bankers?\", \"context\": null}]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_stack.apis.inference import UserMessage\n",
    "import json\n",
    "import random\n",
    "\n",
    "qna_dataset_rows = []\n",
    "\n",
    "chat_completion_input = UserMessage(content=\"\")\n",
    "for i in range(len(qnas_df)):\n",
    "    qna = {}\n",
    "    qna[\"input_query\"] = qnas_df.iloc[i][\"question\"]\n",
    "    qna[\"expected_answer\"] = qnas_df.iloc[i][\"answer\"]\n",
    "\n",
    "    chat_completion_input.content = qna[\"input_query\"]\n",
    "    qna[\"chat_completion_input\"] = json.dumps([chat_completion_input.model_dump()])\n",
    "\n",
    "    qna_dataset_rows.append(qna)\n",
    "\n",
    "random.shuffle(qna_dataset_rows)\n",
    "qna_dataset_df = pd.DataFrame(qna_dataset_rows)\n",
    "qna_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fd5734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered dataset **test_dataset_23500c84-25c6-11f0-b6aa-4a70c355aff9**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_dataset_id = f\"test_dataset_{uuid.uuid1()}\"\n",
    "_ = client.datasets.register(\n",
    "    purpose=\"eval/messages-answer\",\n",
    "    source={\n",
    "        \"type\": \"rows\",\n",
    "        \"rows\": qna_dataset_rows,\n",
    "    },\n",
    "    dataset_id=qna_dataset_id,\n",
    ")\n",
    "display_markdown(f\"Registered dataset **{qna_dataset_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b669a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered benchmark **test_benchmark_237f9094-25c6-11f0-b6aa-4a70c355aff9**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_benchmark_id = f\"test_benchmark_{uuid.uuid1()}\"\n",
    "client.benchmarks.register(\n",
    "    benchmark_id=qna_benchmark_id,\n",
    "    dataset_id=qna_dataset_id,\n",
    "    scoring_functions=[\"llm-as-judge::405b-simpleqa\"],\n",
    ")\n",
    "display_markdown(f\"Registered benchmark **{qna_benchmark_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd43d5",
   "metadata": {},
   "source": [
    "## 5. LLM Eval without RAG\n",
    "- Create an agent configuration without the `knowledge_search` tool.\n",
    "- Run the evaluation function with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "451b9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is completed\n",
      "Evaluation of 50 Q&A workflows completed in 367.753 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Computed accuracy is 38.0%**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "vanilla_responses = {}\n",
    "_job = _run_eval(use_rag=False)\n",
    "# pprint(_job)\n",
    "_eval_response = _get_eval_reponse(_job)\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "vanilla_responses[model_id] = _eval_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe7cd9",
   "metadata": {},
   "source": [
    "## 6. LLM Eval with RAG\n",
    "- Create an agent configuration with the `knowledge_search` tool.\n",
    "- Run the evaluation function with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b7402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is completed\n",
      "Evaluation of 50 Q&A workflows completed in 276.377 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Computed accuracy is 62.0%**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**RAG knowledge search tool used in 41 of (50) agentic calls**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "rag_responses = {}\n",
    "_job = _run_eval(use_rag=True)\n",
    "# pprint(_job)\n",
    "_eval_response = _get_eval_reponse(_job)\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "rag_responses[model_id] = _eval_response\n",
    "\n",
    "retrieved_contexts = sum(\n",
    "    [1 for r in rag_responses[model_id].generations if \"context\" in r]\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**RAG knowledge search tool used in {retrieved_contexts} of ({MAX_QNA_ROWS}) agentic calls**\",\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 4. Reporting\n",
    "- Aggregated accuracy.\n",
    "- Individual scores and responses.\n",
    "- Statistical Significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df0d8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7b35a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7b35a_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_7b35a_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_7b35a_level0_col2\" class=\"col_heading level0 col2\" >RAG Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7b35a_row0_col0\" class=\"data row0 col0\" >granite32-8b</td>\n",
       "      <td id=\"T_7b35a_row0_col1\" class=\"data row0 col1\" >38.000000</td>\n",
       "      <td id=\"T_7b35a_row0_col2\" class=\"data row0 col2\" >62.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x124394ad0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_responses = {}\n",
    "pd_responses['questions'] = [qna_dataset_rows[i]['input_query'] for i in range(MAX_QNA_ROWS)]\n",
    "pd_responses['expected'] = [qna_dataset_rows[i]['expected_answer'] for i in range(MAX_QNA_ROWS)]\n",
    "\n",
    "pd_accuracies = {}\n",
    "df_accuracies = pd.DataFrame.from_dict({\n",
    "    'Model': vanilla_responses.keys(),\n",
    "    'Accuracy': [accuracy_from_categorical_count(vanilla_responses[model_id]) for model_id in vanilla_responses.keys()],\n",
    "    'RAG Accuracy': [accuracy_from_categorical_count(rag_responses[model_id]) for model_id in rag_responses.keys()]})\n",
    "df_accuracies.style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95b9baa2-4739-426a-b79a-2ff90f44c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_data = {}\n",
    "ratings_data = {}\n",
    "responses_data = {}\n",
    "\n",
    "report_data['Question'] = [qna_dataset_rows[i]['input_query'] for i in range(MAX_QNA_ROWS)]\n",
    "ratings_data['Question'] = report_data['Question']\n",
    "responses_data['Question'] = report_data['Question']\n",
    "report_data['Expected Answer'] = [qna_dataset_rows[i]['expected_answer'] for i in range(MAX_QNA_ROWS)]\n",
    "responses_data['Expected Answer'] = report_data['Expected Answer']\n",
    "for model_id in vanilla_responses.keys():\n",
    "    report_data[f'{model_id} Rating'] = [to_label(score_row) for score_row in vanilla_responses[model_id].scores['llm-as-judge::405b-simpleqa'].score_rows]\n",
    "    report_data[f'{model_id} Answer'] = [g['generated_answer'] for g in vanilla_responses[model_id].generations]\n",
    "    report_data[f'{model_id} RAG Rating'] = [to_label(score_row) for score_row in rag_responses[model_id].scores['llm-as-judge::405b-simpleqa'].score_rows]\n",
    "    report_data[f'{model_id} RAG Answer'] = [g['generated_answer'] for g in rag_responses[model_id].generations]\n",
    "    \n",
    "    ratings_data[f'{model_id} Rating'] = report_data[f'{model_id} Rating']\n",
    "    ratings_data[f'{model_id} RAG Rating'] = report_data[f'{model_id} RAG Rating']\n",
    "    responses_data[f'{model_id} Answer'] = report_data[f'{model_id} Answer']\n",
    "    responses_data[f'{model_id} RAG Answer'] = report_data[f'{model_id} RAG Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "272729b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response Comparison**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question</th>\n",
       "      <th>Expected Answer</th>\n",
       "      <th>granite32-8b Rating</th>\n",
       "      <th>granite32-8b Answer</th>\n",
       "      <th>granite32-8b RAG Rating</th>\n",
       "      <th>granite32-8b RAG Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Why might it be beneficial to keep a document with all your answers to the journaling activities?</td>\n",
       "      <td>Keeping a document with all your answers allows you to track your learning and progress, providing a reference point that can help guide your career decisions and development in the financial services industry.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>1. Personal Growth: Journaling is a tool for self-discovery and personal growth. Keeping a document of your answers can help you track your progress, identify patterns, and see how your thoughts and feelings have evolved over time.\\n\\n2. Reflection: It allows for deeper reflection. You can revisit your past entries to understand your thought processes, emotions, and experiences more clearly.\\n\\n3. Learning: It serves as a learning tool. You can review your past answers to understand what worked, what didn't, and how you've grown from your experiences.\\n\\n4. Goal Setting and Achievement: If your journaling activities include setting and tracking goals, having a document of your answers can help you monitor your progress and stay motivated.\\n\\n5. Crisis Management: In times of crisis or stress, revisiting past entries can provide comfort, perspective, and coping strategies.\\n\\n6. Creativity: For creative journaling activities, having a collection of your work can inspire new ideas and help you see your creative evolution.\\n\\n7. Accountability: It holds you accountable. Seeing your past commitments, plans, or promises in writing can motivate you to follow through.\\n\\n8. Research: If your journaling activities involve research or learning new topics, your document can serve as a reference or study guide.\\n\\n9. Therapeutic: For those using journaling as a form of therapy, a collection of entries can provide a comprehensive view of their mental health journey.\\n\\n10. Sharing: If you choose to share your journaling with a therapist, coach, or close friend, having a document readily available makes this process easier.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Keeping a document with all your answers to journaling activities can be beneficial for several reasons:\\n\\n1. **Review and Reflection**: It allows you to revisit your thoughts, ideas, and responses at a later time. This can be particularly useful for self-reflection and understanding your personal growth or changes in perspective over time.\\n\\n2. **Consistency**: It ensures consistency in your responses, especially if you're working on a series of activities or assignments. You can refer back to your previous answers to maintain a coherent narrative or approach.\\n\\n3. **Learning and Improvement**: By reviewing your past answers, you can identify areas where you've improved or where you might need further study. This can be a valuable tool for learning and skill development.\\n\\n4. **Time-Saving**: If you need to revisit a particular topic or question, having a document with your previous answers can save you time that would otherwise be spent recalling or re-researching the information.\\n\\n5. **Quality Assurance**: In a professional setting, such as customer service or banking, maintaining a record of your interactions and responses can serve as a quality assurance tool. It can help in training, performance evaluation, and ensuring consistent, high-quality service.\\n\\n6. **Compliance and Documentation**: In certain industries, maintaining records of interactions and decisions is not only beneficial but also required by law or company policy. This can help in demonstrating compliance and providing necessary documentation.\\n\\nIn the context of the provided documents, maintaining such a document aligns with the emphasis on accuracy, quality service, and continuous learning and improvement, all of which are highlighted in the resources.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is the name of the training program implemented by Wealth Management Specialists?</td>\n",
       "      <td>Elite Growth Practice (EGP)</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>The specific training program implemented by Wealth Management Specialists can vary depending on the company or individual specialist. However, a well-known and widely recognized training program in the wealth management industry is the Chartered Financial Analyst (CFA) program. This is a professional designation offered by the CFA Institute.\\n\\nOther notable training programs include:\\n\\n1. Certified Financial Planner (CFP) - offered by the Certified Financial Planner Board of Standards, Inc.\\n2. Financial Risk Manager (FRM) - offered by the Global Association of Risk Professionals (GARP)\\n3. Chartered Investment Management Analyst (CIMA) - offered by the CAIA Association\\n\\nThese programs provide comprehensive training in various aspects of wealth management, including investment management, financial planning, and risk management. \\n\\nPlease note that not all wealth management specialists may undergo these specific programs, as there are many paths to becoming a wealth management professional. Some may have degrees in finance, economics, or related fields, and gain experience through on-the-job training or other professional development opportunities.</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>Based on the provided documents, there is no specific training program named by Wealth Management Specialists. The documents discuss various roles within Parasol Financial Corporation, including Client Management, Financial Advisory, Investment Banking, Sales and Trading, Accounting, Credit, Wealth Management, Client Services, Technology &amp; Operations, and Business Support Services. However, there is no explicit mention of a training program specifically for Wealth Management Specialists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What should you text to opt out of all security alerts?</td>\n",
       "      <td>You should text STOP to any of the short codes to opt out of all security alerts.</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>I'm sorry for any confusion, but it's important to clarify that opting out of all security alerts is generally not recommended due to the critical role these alerts play in maintaining the security of your accounts and devices. They provide crucial information about potential threats or unusual activities.\\n\\nHowever, if you're dealing with a specific service or application and you want to reduce the frequency of alerts, you should look for an option within the settings or preferences of that service. The exact process can vary greatly depending on the service provider.\\n\\nFor example, in some banking apps, you might find an option under \"Settings\" or \"Notifications\" to customize the types of alerts you receive. \\n\\nIf you're referring to a specific service or platform, please provide more details so I can give a more accurate response. \\n\\nRemember, it's always a good idea to keep at least some level of security alerts active to stay informed about any suspicious activities.</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>To opt out of all security alerts, you should text STOP to the short code 50014. This information is based on the document provided, which outlines procedures for online banking alerts via text message. Please note that this might vary depending on the specific bank or financial institution. Always refer to your bank's official guidelines for the most accurate information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Why might Parasol Financial include a quiz on the Better Money Habits homepage?</td>\n",
       "      <td>Parasol Financial might include a quiz on the Better Money Habits homepage to tailor the content to the user's specific life stage and interests, thereby making the financial advice more relevant and effective for each individual.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Parasol Financial might include a quiz on the Better Money Habits homepage for several reasons:\\n\\n1. **Engagement**: Quizzes are an engaging way to capture and maintain user interest. They provide an interactive experience that can keep visitors on the site longer.\\n\\n2. **Education**: The quiz can serve as an educational tool, helping users understand their current financial habits and knowledge. This can be particularly useful in a context like Better Money Habits, which aims to improve financial literacy.\\n\\n3. **Personalization**: Based on the quiz results, Parasol Financial can offer personalized financial advice or resources. This tailored approach can make users feel the content is more relevant to their specific needs.\\n\\n4. **Data Collection**: The quiz can also serve as a method to collect data about users' financial behaviors and attitudes. This data can be valuable for research, understanding their audience better, and potentially for marketing purposes.\\n\\n5. **Lead Generation**: By offering valuable insights at the end of the quiz, Parasol Financial can encourage users to provide their contact information, thereby generating leads for their financial services.\\n\\n6. **SEO Benefits**: Quizzes can increase the time users spend on a page, which can positively impact search engine rankings. They also often encourage social sharing, which can drive more traffic to the site.\\n\\nRemember, the specific reasons would depend on Parasol Financial's overall strategy and goals for the Better Money Habits platform.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Parasol Financial might include a quiz on the Better Money Habits homepage to personalize the user's experience. By taking the quiz, users can receive content tailored to their life priorities and financial needs. This approach allows Parasol Financial to provide more relevant information, potentially increasing user engagement and satisfaction. The quiz could help identify topics of interest, such as managing finances for different life stages, which can then guide users to the most suitable resources on the site.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is one of the job expectations for Senior Bankers?</td>\n",
       "      <td>Proactively connecting with clients through outbound calls and conducting consistent follow-up routines.</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>One of the key job expectations for Senior Bankers is to provide strategic leadership and direction to the banking operations. This includes:\\n\\n1. **Developing and Implementing Strategic Plans**: Senior Bankers are expected to create and execute long-term strategies that align with the bank's overall objectives. This involves analyzing market trends, identifying opportunities, and formulating plans to capitalize on them.\\n\\n2. **Risk Management**: They are responsible for overseeing and managing the bank's risk profile. This includes credit risk, market risk, operational risk, and liquidity risk. They need to ensure that the bank's risk-taking activities are within acceptable limits and comply with regulatory requirements.\\n\\n3. **Client Relationship Management**: Senior Bankers often have a significant role in maintaining and growing relationships with key clients. This involves understanding their financial needs, providing tailored solutions, and ensuring client satisfaction.\\n\\n4. **Team Leadership and Development**: They are expected to lead, mentor, and develop their team members. This includes setting performance goals, providing feedback, and fostering a positive work environment.\\n\\n5. **Regulatory Compliance**: Senior Bankers must ensure that all banking activities comply with relevant laws and regulations. This involves staying updated on changes in banking laws and implementing necessary adjustments in the bank's operations.\\n\\n6. **Financial Analysis and Reporting**: They are responsible for analyzing financial data, preparing reports, and making recommendations based on their findings. This could involve assessing the bank's financial health, identifying areas for improvement, and forecasting future trends.\\n\\n7. **Decision Making**: Senior Bankers are often required to make critical decisions that can significantly impact the bank's operations and profitability. This requires a deep understanding of the banking industry, strong analytical skills, and the ability to weigh potential risks and rewards.\\n\\nRemember, the specific job expectations can vary depending on the particular bank and the specific role within the bank.</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>One of the job expectations for Senior Bankers, based on the provided information, includes leveraging problem-solving competencies, demonstrating sound judgment and decision-making, and providing exceptional client service and empathy. They are expected to meet both the bank's and clients' needs, often in a client-facing role within the Banking, Markets &amp; Finance sector. This could involve tasks such as managing client relationships, providing financial advice, or overseeing investment banking activities.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "display_markdown(\"**Response Comparison**\", raw=True)\n",
    "report_df = pd.DataFrame.from_dict(report_data)\n",
    "HTML(report_df.head().to_html(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5409d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Statistical Significance (vanilla Vs RAG generations)**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " granite32-8b                                      :     0.3800\n",
      " p_value                                           :     0.0052\n",
      "\n",
      "p_value<0.05 so this result is statistically significant\n",
      "You can conclude that RAG generation is better on data of this sort\n"
     ]
    }
   ],
   "source": [
    "display_markdown(\"**Statistical Significance (vanilla Vs RAG generations)**\", raw= True)\n",
    "print_stats_significance(numeric_scores(vanilla_responses[model_id]), numeric_scores(rag_responses[model_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This tutorial demonstrates how to evaluate an agentic workflow, without and without RAG tool, using the Llama Stack reference implementation.\n",
    "We do so by initializing an agent, with optional access to the RAG tool, then invoking the agent evaluation against a predefined reference of sample Q&A. \n",
    "Please check out our [complementary tutorial](Level3_agentic_RAG.ipynb) for an agentic RAG example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
